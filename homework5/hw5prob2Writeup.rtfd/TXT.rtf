{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\b\fs24 \cf0 Problem 2
\b0 \
\
For this problem we found the general partial derivatives of (y-be^ax)^2 and then functions to evaluate these functions given x,y,a, and b as arguments and used the sum of these evaluated at our given points as our normal equations.  We made 4 other similar functions to make the elements of the Jacobian matrix.  With these functions we made an inverseJacobian function that returns the inverse of the jocobian for our normal equations.  Then in a while loop we multiplied our inverted Jacobian by a column vector made of our normal equations evaluated at our current guess.  We then subtracted the result of this from our current guess to get our new guess.  Then we used a tolerance function that uses the infinity norm to decide whether or not to break from the loop or continue to another guess.\
\
Our initial guess was a = -1 and b = 1 because from the data points you could tell the function is decreasing so the exponent must be negative and it crosses the y axis near 1 so b must be close to 1 as well.  This gave us the results:\
\
The best fit function is: 0.924420304119e^(-0.655565820815x)\
The sum of the squared errors is: 0.00784352515529\
\
Which is fairly accurate and the graph of our approximation matches pretty closely to the graph of our data points:\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 {{\NeXTGraphic Pasted Graphic.tiff \width16000 \height12000
}¬}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
}